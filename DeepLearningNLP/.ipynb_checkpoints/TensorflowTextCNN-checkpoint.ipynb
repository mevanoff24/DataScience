{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import data_helpers2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, constant=0.1):\n",
    "    initial = tf.constant(value = constant, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "        embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # placeholders\n",
    "        self.x = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        # l2 regularization\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # embedding layer - maps vocabulary word indices into low-dimensional vectors - lookup table\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "            # result of the embedding operation is a 3-dimensional tensor [None, sequence_length, embedding_size]\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.x)\n",
    "            # need to add channel to get [None, sequence_length, embedding_size, 1].\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        # convolution layer\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
    "                # conv\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                # filter matrix\n",
    "                W = weight_variable(filter_shape)\n",
    "                b = bias_variable([num_filters])\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded, W, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "                \n",
    "                hidden = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "                pooled = tf.nn.max_pool(hidden, ksize = [1, sequence_length - filter_size + 1, 1, 1],\n",
    "                                       strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        # combine pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.hidden_pool = tf.concat(3, pooled_outputs)\n",
    "        self.hidden_pool_flat = tf.reshape(self.hidden_pool, [-1, num_filters_total])\n",
    "        \n",
    "        # dropout - “disable” a fraction of its neurons. \n",
    "        with tf.name_scope('dropout'):\n",
    "            self.hidden_drop = tf.nn.dropout(self.hidden_pool_flat, self.keep_prob)\n",
    "            \n",
    "        # output layer\n",
    "        with tf.name_scope('output'):\n",
    "            W = weight_variable([num_filters_total, num_classes])\n",
    "            b = bias_variable([num_classes])\n",
    "            reg = tf.nn.l2_loss(W) + tf.nn.l2_loss(b)\n",
    "            l2_loss += reg\n",
    "            self.scores = tf.nn.xw_plus_b(self.hidden_drop, W, b)\n",
    "            self.prediction = tf.argmax(self.scores, 1)\n",
    "            \n",
    "        # loss\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.y_)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            \n",
    "        # evaluate\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_pred = tf.equal(self.prediction, tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18766\n",
      "Train / Test Split: 8531 / 2133\n",
      "2016-03-01T17:27:50.565629: step 50, loss 2.06868, acc 0.625\n",
      "2016-03-01T17:27:59.469472: step 100, loss 2.01822, acc 0.5\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:28:05.510034: step 101, loss 0.859559, acc 0.514299\n",
      "2016-03-01T17:28:14.133498: step 150, loss 1.74908, acc 0.546875\n",
      "2016-03-01T17:28:22.963825: step 200, loss 2.44967, acc 0.5\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:28:28.846314: step 201, loss 0.797452, acc 0.543835\n",
      "2016-03-01T17:28:39.624617: step 250, loss 1.59957, acc 0.515625\n",
      "2016-03-01T17:28:50.393001: step 300, loss 1.94203, acc 0.515625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:28:57.415989: step 301, loss 0.77571, acc 0.5579\n",
      "2016-03-01T17:29:07.422226: step 350, loss 1.86289, acc 0.625\n",
      "2016-03-01T17:29:17.467626: step 400, loss 1.96155, acc 0.46875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:29:24.317467: step 401, loss 0.76082, acc 0.568214\n",
      "2016-03-01T17:29:35.291509: step 450, loss 2.2516, acc 0.453125\n",
      "2016-03-01T17:29:46.391725: step 500, loss 1.53817, acc 0.609375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:29:53.886725: step 501, loss 0.743709, acc 0.586029\n",
      "2016-03-01T17:30:03.758408: step 550, loss 1.11367, acc 0.734375\n",
      "2016-03-01T17:30:14.501933: step 600, loss 1.35474, acc 0.546875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:30:21.472745: step 601, loss 0.742944, acc 0.585091\n",
      "2016-03-01T17:30:32.279542: step 650, loss 1.68595, acc 0.53125\n",
      "2016-03-01T17:30:43.864738: step 700, loss 1.62769, acc 0.5\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:30:51.492904: step 701, loss 0.698268, acc 0.609001\n",
      "2016-03-01T17:31:03.059474: step 750, loss 1.57434, acc 0.625\n",
      "2016-03-01T17:31:14.391443: step 800, loss 1.32365, acc 0.578125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:31:20.238737: step 801, loss 0.691831, acc 0.616971\n",
      "2016-03-01T17:31:29.602470: step 850, loss 1.39357, acc 0.59375\n",
      "2016-03-01T17:31:39.637798: step 900, loss 1.15506, acc 0.640625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:31:45.576856: step 901, loss 0.676656, acc 0.632911\n",
      "2016-03-01T17:31:54.886552: step 950, loss 1.26459, acc 0.578125\n",
      "2016-03-01T17:32:04.541812: step 1000, loss 1.21177, acc 0.578125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:32:10.369818: step 1001, loss 0.665017, acc 0.639944\n",
      "2016-03-01T17:32:19.038275: step 1050, loss 1.33659, acc 0.53125\n",
      "2016-03-01T17:32:27.715635: step 1100, loss 0.991545, acc 0.640625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:32:33.865528: step 1101, loss 0.667814, acc 0.631974\n",
      "2016-03-01T17:32:42.527843: step 1150, loss 1.03572, acc 0.640625\n",
      "2016-03-01T17:32:51.377815: step 1200, loss 1.34943, acc 0.5625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:32:57.455634: step 1201, loss 0.646323, acc 0.657759\n",
      "2016-03-01T17:33:05.985307: step 1250, loss 1.50609, acc 0.5\n",
      "2016-03-01T17:33:14.737594: step 1300, loss 1.05725, acc 0.671875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:33:20.504925: step 1301, loss 0.639623, acc 0.661041\n",
      "2016-03-01T17:33:29.219527: step 1350, loss 1.04005, acc 0.65625\n",
      "2016-03-01T17:33:38.119515: step 1400, loss 1.0408, acc 0.609375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:33:43.972712: step 1401, loss 0.632931, acc 0.660103\n",
      "2016-03-01T17:33:52.681490: step 1450, loss 1.16032, acc 0.609375\n",
      "2016-03-01T17:34:01.417164: step 1500, loss 1.20865, acc 0.53125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:34:07.183377: step 1501, loss 0.624965, acc 0.668073\n",
      "2016-03-01T17:34:15.799990: step 1550, loss 0.954983, acc 0.640625\n",
      "2016-03-01T17:34:24.598538: step 1600, loss 0.93458, acc 0.625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:34:30.730996: step 1601, loss 0.623107, acc 0.672761\n",
      "2016-03-01T17:34:39.324334: step 1650, loss 0.931552, acc 0.59375\n",
      "2016-03-01T17:34:48.187379: step 1700, loss 0.965228, acc 0.59375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:34:54.123506: step 1701, loss 0.613575, acc 0.674637\n",
      "2016-03-01T17:35:02.908211: step 1750, loss 1.16608, acc 0.546875\n",
      "2016-03-01T17:35:11.651201: step 1800, loss 0.996068, acc 0.640625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:35:17.712120: step 1801, loss 0.610149, acc 0.684013\n",
      "2016-03-01T17:35:26.512960: step 1850, loss 1.35353, acc 0.484375\n",
      "2016-03-01T17:35:35.266169: step 1900, loss 0.828806, acc 0.640625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:35:41.115993: step 1901, loss 0.602519, acc 0.686357\n",
      "2016-03-01T17:35:49.793211: step 1950, loss 1.17962, acc 0.609375\n",
      "2016-03-01T17:35:58.698903: step 2000, loss 0.887643, acc 0.5625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:36:04.716069: step 2001, loss 0.600305, acc 0.68542\n",
      "2016-03-01T17:36:13.287642: step 2050, loss 1.06775, acc 0.578125\n",
      "2016-03-01T17:36:22.051039: step 2100, loss 0.793658, acc 0.6875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:36:27.963248: step 2101, loss 0.594283, acc 0.695265\n",
      "2016-03-01T17:36:36.678008: step 2150, loss 0.884739, acc 0.6875\n",
      "2016-03-01T17:36:45.393729: step 2200, loss 0.754934, acc 0.671875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:36:51.468205: step 2201, loss 0.589812, acc 0.697609\n",
      "2016-03-01T17:37:00.165009: step 2250, loss 1.0406, acc 0.609375\n",
      "2016-03-01T17:37:09.173173: step 2300, loss 0.354319, acc 0.789474\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:37:14.896149: step 2301, loss 0.586008, acc 0.698547\n",
      "2016-03-01T17:37:23.533720: step 2350, loss 1.15212, acc 0.5\n",
      "2016-03-01T17:37:32.349470: step 2400, loss 0.922532, acc 0.625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:37:38.279965: step 2401, loss 0.582715, acc 0.701828\n",
      "2016-03-01T17:37:46.892307: step 2450, loss 1.00291, acc 0.578125\n",
      "2016-03-01T17:37:55.847865: step 2500, loss 0.793739, acc 0.6875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:38:01.519709: step 2501, loss 0.578571, acc 0.707923\n",
      "2016-03-01T17:38:10.175046: step 2550, loss 0.754304, acc 0.640625\n",
      "2016-03-01T17:38:18.995200: step 2600, loss 0.956056, acc 0.53125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:38:24.956410: step 2601, loss 0.576361, acc 0.70511\n",
      "2016-03-01T17:38:33.688383: step 2650, loss 0.811396, acc 0.71875\n",
      "2016-03-01T17:38:42.565253: step 2700, loss 0.600194, acc 0.65625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:38:48.482146: step 2701, loss 0.573445, acc 0.70511\n",
      "2016-03-01T17:38:57.080824: step 2750, loss 0.765583, acc 0.625\n",
      "2016-03-01T17:39:06.019441: step 2800, loss 0.853448, acc 0.65625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:39:11.986346: step 2801, loss 0.583432, acc 0.695734\n",
      "2016-03-01T17:39:20.528768: step 2850, loss 0.655315, acc 0.671875\n",
      "2016-03-01T17:39:29.318723: step 2900, loss 0.602892, acc 0.671875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:39:35.263007: step 2901, loss 0.566695, acc 0.717768\n",
      "2016-03-01T17:39:43.933372: step 2950, loss 0.603792, acc 0.6875\n",
      "2016-03-01T17:39:52.743051: step 3000, loss 0.735465, acc 0.625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:39:58.630610: step 3001, loss 0.582619, acc 0.691045\n",
      "2016-03-01T17:40:07.290420: step 3050, loss 0.642711, acc 0.6875\n",
      "2016-03-01T17:40:16.189637: step 3100, loss 0.497748, acc 0.765625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:40:22.307025: step 3101, loss 0.56302, acc 0.715424\n",
      "2016-03-01T17:40:30.999464: step 3150, loss 0.4733, acc 0.78125\n",
      "2016-03-01T17:40:39.893536: step 3200, loss 0.669812, acc 0.640625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:40:45.788997: step 3201, loss 0.56067, acc 0.718237\n",
      "2016-03-01T17:40:54.438216: step 3250, loss 0.804055, acc 0.609375\n",
      "2016-03-01T17:41:03.242302: step 3300, loss 0.747869, acc 0.609375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:41:09.299713: step 3301, loss 0.560339, acc 0.718706\n",
      "2016-03-01T17:41:17.967634: step 3350, loss 0.536775, acc 0.71875\n",
      "2016-03-01T17:41:26.743631: step 3400, loss 0.694421, acc 0.671875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:41:32.807210: step 3401, loss 0.55669, acc 0.724332\n",
      "2016-03-01T17:41:41.586259: step 3450, loss 0.730462, acc 0.609375\n",
      "2016-03-01T17:41:50.427385: step 3500, loss 0.724894, acc 0.65625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:41:56.305440: step 3501, loss 0.554862, acc 0.725738\n",
      "2016-03-01T17:42:04.873209: step 3550, loss 0.649936, acc 0.703125\n",
      "2016-03-01T17:42:13.702248: step 3600, loss 0.432982, acc 0.78125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:42:19.541874: step 3601, loss 0.565008, acc 0.70511\n",
      "2016-03-01T17:42:28.271679: step 3650, loss 0.596121, acc 0.734375\n",
      "2016-03-01T17:42:37.895498: step 3700, loss 0.719107, acc 0.703125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:42:43.559179: step 3701, loss 0.551906, acc 0.726207\n",
      "2016-03-01T17:42:52.225887: step 3750, loss 0.527935, acc 0.734375\n",
      "2016-03-01T17:43:01.029416: step 3800, loss 0.5261, acc 0.734375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:43:06.976243: step 3801, loss 0.559569, acc 0.716362\n",
      "2016-03-01T17:43:15.584488: step 3850, loss 0.635719, acc 0.703125\n",
      "2016-03-01T17:43:24.378663: step 3900, loss 0.617045, acc 0.734375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:43:30.284893: step 3901, loss 0.547946, acc 0.729958\n",
      "2016-03-01T17:43:38.941368: step 3950, loss 0.540146, acc 0.734375\n",
      "2016-03-01T17:43:47.739374: step 4000, loss 0.401186, acc 0.828125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:43:53.697336: step 4001, loss 0.546989, acc 0.732771\n",
      "2016-03-01T17:44:02.330804: step 4050, loss 0.43501, acc 0.84375\n",
      "2016-03-01T17:44:11.135251: step 4100, loss 0.363725, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:44:17.202397: step 4101, loss 0.544456, acc 0.735584\n",
      "2016-03-01T17:44:25.822536: step 4150, loss 0.504258, acc 0.765625\n",
      "2016-03-01T17:44:34.552440: step 4200, loss 0.601335, acc 0.671875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:44:40.426709: step 4201, loss 0.543631, acc 0.735584\n",
      "2016-03-01T17:44:49.246057: step 4250, loss 0.463652, acc 0.84375\n",
      "2016-03-01T17:44:58.023096: step 4300, loss 0.535402, acc 0.71875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:45:04.069275: step 4301, loss 0.541162, acc 0.737459\n",
      "2016-03-01T17:45:12.710976: step 4350, loss 0.611821, acc 0.6875\n",
      "2016-03-01T17:45:21.547587: step 4400, loss 0.572871, acc 0.734375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:45:27.432345: step 4401, loss 0.543131, acc 0.731364\n",
      "2016-03-01T17:45:36.130945: step 4450, loss 0.630753, acc 0.703125\n",
      "2016-03-01T17:45:44.889603: step 4500, loss 0.586088, acc 0.6875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:45:50.713516: step 4501, loss 0.537524, acc 0.737459\n",
      "2016-03-01T17:45:59.403417: step 4550, loss 0.461472, acc 0.78125\n",
      "2016-03-01T17:46:08.292612: step 4600, loss 0.603835, acc 0.671875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:46:14.352875: step 4601, loss 0.536062, acc 0.745429\n",
      "2016-03-01T17:46:22.920507: step 4650, loss 0.395089, acc 0.859375\n",
      "2016-03-01T17:46:31.720981: step 4700, loss 0.451022, acc 0.765625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:46:37.598726: step 4701, loss 0.533501, acc 0.74121\n",
      "2016-03-01T17:46:46.218948: step 4750, loss 0.422035, acc 0.84375\n",
      "2016-03-01T17:46:55.040817: step 4800, loss 0.457606, acc 0.796875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:47:00.962887: step 4801, loss 0.533355, acc 0.74496\n",
      "2016-03-01T17:47:09.579911: step 4850, loss 0.46725, acc 0.734375\n",
      "2016-03-01T17:47:18.241546: step 4900, loss 0.531278, acc 0.703125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:47:24.323973: step 4901, loss 0.530607, acc 0.743554\n",
      "2016-03-01T17:47:33.058063: step 4950, loss 0.47121, acc 0.828125\n",
      "2016-03-01T17:47:41.867915: step 5000, loss 0.468438, acc 0.765625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:47:47.583851: step 5001, loss 0.528201, acc 0.750586\n",
      "2016-03-01T17:47:56.130348: step 5050, loss 0.479424, acc 0.71875\n",
      "2016-03-01T17:48:04.965307: step 5100, loss 0.469284, acc 0.828125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:48:10.732183: step 5101, loss 0.528943, acc 0.745429\n",
      "2016-03-01T17:48:19.270257: step 5150, loss 0.521652, acc 0.734375\n",
      "2016-03-01T17:48:28.097686: step 5200, loss 0.55786, acc 0.6875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:48:34.167855: step 5201, loss 0.529357, acc 0.737459\n",
      "2016-03-01T17:48:42.809796: step 5250, loss 0.469416, acc 0.765625\n",
      "2016-03-01T17:48:51.505705: step 5300, loss 0.37864, acc 0.84375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:48:57.523033: step 5301, loss 0.521679, acc 0.751524\n",
      "2016-03-01T17:49:06.194379: step 5350, loss 0.455151, acc 0.75\n",
      "2016-03-01T17:49:15.031476: step 5400, loss 0.475336, acc 0.8125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:49:20.868903: step 5401, loss 0.519237, acc 0.755274\n",
      "2016-03-01T17:49:29.583592: step 5450, loss 0.376797, acc 0.796875\n",
      "2016-03-01T17:49:38.382829: step 5500, loss 0.47117, acc 0.78125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:49:44.290504: step 5501, loss 0.516403, acc 0.759025\n",
      "2016-03-01T17:49:53.498813: step 5550, loss 0.353006, acc 0.84375\n",
      "2016-03-01T17:50:02.279615: step 5600, loss 0.430474, acc 0.84375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:50:08.095996: step 5601, loss 0.516843, acc 0.755274\n",
      "2016-03-01T17:50:16.686598: step 5650, loss 0.409454, acc 0.796875\n",
      "2016-03-01T17:50:25.341185: step 5700, loss 0.449171, acc 0.8125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:50:31.544182: step 5701, loss 0.511248, acc 0.756212\n",
      "2016-03-01T17:50:40.136220: step 5750, loss 0.37538, acc 0.859375\n",
      "2016-03-01T17:50:49.056121: step 5800, loss 0.494221, acc 0.734375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:50:55.080199: step 5801, loss 0.510237, acc 0.762307\n",
      "2016-03-01T17:51:03.698085: step 5850, loss 0.408984, acc 0.828125\n",
      "2016-03-01T17:51:12.494455: step 5900, loss 0.33307, acc 0.875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:51:18.283950: step 5901, loss 0.506638, acc 0.765588\n",
      "2016-03-01T17:51:26.958388: step 5950, loss 0.393358, acc 0.828125\n",
      "2016-03-01T17:51:35.743555: step 6000, loss 0.560668, acc 0.71875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:51:41.667217: step 6001, loss 0.50461, acc 0.766057\n",
      "2016-03-01T17:51:50.422968: step 6050, loss 0.421156, acc 0.796875\n",
      "2016-03-01T17:51:59.158216: step 6100, loss 0.388946, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:52:05.224329: step 6101, loss 0.503328, acc 0.764182\n",
      "2016-03-01T17:52:13.866247: step 6150, loss 0.358086, acc 0.828125\n",
      "2016-03-01T17:52:22.599241: step 6200, loss 0.287135, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:52:28.444755: step 6201, loss 0.500187, acc 0.766526\n",
      "2016-03-01T17:52:37.008826: step 6250, loss 0.352886, acc 0.859375\n",
      "2016-03-01T17:52:45.851694: step 6300, loss 0.312639, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:52:51.734820: step 6301, loss 0.498432, acc 0.76512\n",
      "2016-03-01T17:53:00.394538: step 6350, loss 0.325751, acc 0.859375\n",
      "2016-03-01T17:53:09.124213: step 6400, loss 0.294201, acc 0.875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:53:15.303919: step 6401, loss 0.499515, acc 0.763713\n",
      "2016-03-01T17:53:23.925627: step 6450, loss 0.395975, acc 0.84375\n",
      "2016-03-01T17:53:32.630036: step 6500, loss 0.281008, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:53:38.425542: step 6501, loss 0.490649, acc 0.769339\n",
      "2016-03-01T17:53:47.204903: step 6550, loss 0.324654, acc 0.90625\n",
      "2016-03-01T17:53:56.061906: step 6600, loss 0.380036, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:54:02.125488: step 6601, loss 0.496808, acc 0.761369\n",
      "2016-03-01T17:54:10.677847: step 6650, loss 0.342129, acc 0.84375\n",
      "2016-03-01T17:54:19.539663: step 6700, loss 0.411642, acc 0.8125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:54:25.473070: step 6701, loss 0.485118, acc 0.774496\n",
      "2016-03-01T17:54:34.185832: step 6750, loss 0.354064, acc 0.84375\n",
      "2016-03-01T17:54:42.893759: step 6800, loss 0.312299, acc 0.828125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:54:48.701532: step 6801, loss 0.489948, acc 0.77309\n",
      "2016-03-01T17:54:57.363526: step 6850, loss 0.318787, acc 0.859375\n",
      "2016-03-01T17:55:06.105917: step 6900, loss 0.387393, acc 0.84375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:55:12.209419: step 6901, loss 0.479472, acc 0.785748\n",
      "2016-03-01T17:55:20.845176: step 6950, loss 0.239579, acc 0.890625\n",
      "2016-03-01T17:55:29.710287: step 7000, loss 0.367738, acc 0.84375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:55:35.725572: step 7001, loss 0.477407, acc 0.779653\n",
      "2016-03-01T17:55:44.324662: step 7050, loss 0.368656, acc 0.828125\n",
      "2016-03-01T17:55:53.224667: step 7100, loss 0.404972, acc 0.8125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:55:59.271317: step 7101, loss 0.478041, acc 0.774027\n",
      "2016-03-01T17:56:08.002243: step 7150, loss 0.36606, acc 0.875\n",
      "2016-03-01T17:56:16.691817: step 7200, loss 0.412724, acc 0.78125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:56:22.560586: step 7201, loss 0.471524, acc 0.785279\n",
      "2016-03-01T17:56:31.322710: step 7250, loss 0.372801, acc 0.875\n",
      "2016-03-01T17:56:40.137156: step 7300, loss 0.342781, acc 0.875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:56:45.858283: step 7301, loss 0.476805, acc 0.774027\n",
      "2016-03-01T17:56:54.533242: step 7350, loss 0.312334, acc 0.890625\n",
      "2016-03-01T17:57:03.292898: step 7400, loss 0.412152, acc 0.796875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:57:09.428377: step 7401, loss 0.466251, acc 0.788561\n",
      "2016-03-01T17:57:17.956776: step 7450, loss 0.358689, acc 0.8125\n",
      "2016-03-01T17:57:26.839181: step 7500, loss 0.327009, acc 0.875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:57:32.963637: step 7501, loss 0.463902, acc 0.790436\n",
      "2016-03-01T17:57:41.642346: step 7550, loss 0.339548, acc 0.890625\n",
      "2016-03-01T17:57:50.295474: step 7600, loss 0.258556, acc 0.875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:57:56.149493: step 7601, loss 0.461439, acc 0.78903\n",
      "2016-03-01T17:58:04.886976: step 7650, loss 0.327376, acc 0.875\n",
      "2016-03-01T17:58:13.717025: step 7700, loss 0.375584, acc 0.828125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:58:19.454619: step 7701, loss 0.458472, acc 0.791374\n",
      "2016-03-01T17:58:28.021024: step 7750, loss 0.41178, acc 0.765625\n",
      "2016-03-01T17:58:36.967746: step 7800, loss 0.263626, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:58:42.679713: step 7801, loss 0.455844, acc 0.794187\n",
      "2016-03-01T17:58:51.188777: step 7850, loss 0.22987, acc 0.894737\n",
      "2016-03-01T17:59:00.126971: step 7900, loss 0.239208, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:59:06.103187: step 7901, loss 0.453831, acc 0.795124\n",
      "2016-03-01T17:59:14.771511: step 7950, loss 0.301332, acc 0.90625\n",
      "2016-03-01T17:59:23.521971: step 8000, loss 0.328884, acc 0.84375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:59:29.356303: step 8001, loss 0.456742, acc 0.785748\n",
      "2016-03-01T17:59:38.019648: step 8050, loss 0.213036, acc 0.90625\n",
      "2016-03-01T17:59:46.827165: step 8100, loss 0.279521, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T17:59:53.659666: step 8101, loss 0.449693, acc 0.795124\n",
      "2016-03-01T18:00:04.297973: step 8150, loss 0.312371, acc 0.875\n",
      "2016-03-01T18:00:15.721403: step 8200, loss 0.273786, acc 0.84375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:00:21.883346: step 8201, loss 0.452246, acc 0.788092\n",
      "2016-03-01T18:00:30.771897: step 8250, loss 0.238651, acc 0.921875\n",
      "2016-03-01T18:00:39.523278: step 8300, loss 0.31977, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:00:45.284678: step 8301, loss 0.443651, acc 0.798406\n",
      "2016-03-01T18:00:54.006132: step 8350, loss 0.333801, acc 0.84375\n",
      "2016-03-01T18:01:02.718321: step 8400, loss 0.264877, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:01:08.778040: step 8401, loss 0.441687, acc 0.799812\n",
      "2016-03-01T18:01:17.461729: step 8450, loss 0.154185, acc 0.953125\n",
      "2016-03-01T18:01:26.224307: step 8500, loss 0.235572, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:01:32.220394: step 8501, loss 0.439306, acc 0.801688\n",
      "2016-03-01T18:01:40.805490: step 8550, loss 0.301927, acc 0.875\n",
      "2016-03-01T18:01:49.698013: step 8600, loss 0.207581, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:01:55.698894: step 8601, loss 0.436211, acc 0.799812\n",
      "2016-03-01T18:02:04.393550: step 8650, loss 0.261195, acc 0.859375\n",
      "2016-03-01T18:02:13.068418: step 8700, loss 0.195767, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:02:18.900495: step 8701, loss 0.43327, acc 0.805438\n",
      "2016-03-01T18:02:27.574136: step 8750, loss 0.3225, acc 0.890625\n",
      "2016-03-01T18:02:36.334179: step 8800, loss 0.205493, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:02:42.368584: step 8801, loss 0.436993, acc 0.804501\n",
      "2016-03-01T18:02:51.021518: step 8850, loss 0.278904, acc 0.875\n",
      "2016-03-01T18:02:59.776143: step 8900, loss 0.223124, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:03:05.653906: step 8901, loss 0.428441, acc 0.806376\n",
      "2016-03-01T18:03:14.237819: step 8950, loss 0.339037, acc 0.828125\n",
      "2016-03-01T18:03:23.080755: step 9000, loss 0.264581, acc 0.859375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:03:29.466283: step 9001, loss 0.427565, acc 0.804032\n",
      "2016-03-01T18:03:38.777658: step 9050, loss 0.265378, acc 0.875\n",
      "2016-03-01T18:03:47.500237: step 9100, loss 0.196835, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:03:53.389713: step 9101, loss 0.425148, acc 0.81294\n",
      "2016-03-01T18:04:01.961832: step 9150, loss 0.180871, acc 0.96875\n",
      "2016-03-01T18:04:10.788437: step 9200, loss 0.269584, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:04:16.586744: step 9201, loss 0.423422, acc 0.812471\n",
      "2016-03-01T18:04:25.159726: step 9250, loss 0.187295, acc 0.9375\n",
      "2016-03-01T18:04:33.941044: step 9300, loss 0.285739, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:04:39.803718: step 9301, loss 0.421253, acc 0.812002\n",
      "2016-03-01T18:04:48.430392: step 9350, loss 0.213459, acc 0.90625\n",
      "2016-03-01T18:04:57.264049: step 9400, loss 0.189219, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:05:03.310173: step 9401, loss 0.41878, acc 0.813408\n",
      "2016-03-01T18:05:11.978230: step 9450, loss 0.185006, acc 0.90625\n",
      "2016-03-01T18:05:20.639862: step 9500, loss 0.232671, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:05:26.860482: step 9501, loss 0.415791, acc 0.815752\n",
      "2016-03-01T18:05:35.596480: step 9550, loss 0.269232, acc 0.875\n",
      "2016-03-01T18:05:44.453927: step 9600, loss 0.234499, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:05:50.301721: step 9601, loss 0.413861, acc 0.817628\n",
      "2016-03-01T18:05:58.889395: step 9650, loss 0.215808, acc 0.90625\n",
      "2016-03-01T18:06:07.681042: step 9700, loss 0.20553, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:06:13.399803: step 9701, loss 0.41388, acc 0.818565\n",
      "2016-03-01T18:06:21.988561: step 9750, loss 0.284907, acc 0.921875\n",
      "2016-03-01T18:06:30.797800: step 9800, loss 0.147505, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:06:36.499489: step 9801, loss 0.410566, acc 0.819503\n",
      "2016-03-01T18:06:45.228911: step 9850, loss 0.231155, acc 0.890625\n",
      "2016-03-01T18:06:53.964446: step 9900, loss 0.299623, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:06:59.738984: step 9901, loss 0.40892, acc 0.817628\n",
      "2016-03-01T18:07:08.439903: step 9950, loss 0.232935, acc 0.90625\n",
      "2016-03-01T18:07:17.250489: step 10000, loss 0.0980364, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:07:23.381775: step 10001, loss 0.407002, acc 0.819503\n",
      "2016-03-01T18:07:31.927590: step 10050, loss 0.182435, acc 0.921875\n",
      "2016-03-01T18:07:40.758339: step 10100, loss 0.236328, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:07:46.564857: step 10101, loss 0.404292, acc 0.826535\n",
      "2016-03-01T18:07:55.245452: step 10150, loss 0.181935, acc 0.953125\n",
      "2016-03-01T18:08:03.985043: step 10200, loss 0.132817, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:08:09.809600: step 10201, loss 0.402449, acc 0.827942\n",
      "2016-03-01T18:08:18.459202: step 10250, loss 0.182146, acc 0.9375\n",
      "2016-03-01T18:08:27.142018: step 10300, loss 0.167737, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:08:33.206319: step 10301, loss 0.400701, acc 0.829348\n",
      "2016-03-01T18:08:41.807090: step 10350, loss 0.276499, acc 0.859375\n",
      "2016-03-01T18:08:50.622742: step 10400, loss 0.166091, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:08:56.796756: step 10401, loss 0.400098, acc 0.830286\n",
      "2016-03-01T18:09:05.378297: step 10450, loss 0.0837089, acc 0.96875\n",
      "2016-03-01T18:09:14.150352: step 10500, loss 0.221339, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:09:20.235665: step 10501, loss 0.396908, acc 0.833568\n",
      "2016-03-01T18:09:28.820830: step 10550, loss 0.091833, acc 0.984375\n",
      "2016-03-01T18:09:37.566078: step 10600, loss 0.186035, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:09:43.484349: step 10601, loss 0.396008, acc 0.827004\n",
      "2016-03-01T18:09:52.201173: step 10650, loss 0.188311, acc 0.953125\n",
      "2016-03-01T18:10:00.923486: step 10700, loss 0.101525, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:10:06.797716: step 10701, loss 0.392678, acc 0.831692\n",
      "2016-03-01T18:10:15.433470: step 10750, loss 0.224231, acc 0.9375\n",
      "2016-03-01T18:10:24.294384: step 10800, loss 0.166349, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:10:30.268482: step 10801, loss 0.39118, acc 0.837318\n",
      "2016-03-01T18:10:38.819649: step 10850, loss 0.11603, acc 0.953125\n",
      "2016-03-01T18:10:47.652178: step 10900, loss 0.151939, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:10:53.473824: step 10901, loss 0.390892, acc 0.831692\n",
      "2016-03-01T18:11:02.181293: step 10950, loss 0.138234, acc 0.953125\n",
      "2016-03-01T18:11:10.896675: step 11000, loss 0.164865, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:11:16.644410: step 11001, loss 0.386144, acc 0.837318\n",
      "2016-03-01T18:11:25.317369: step 11050, loss 0.12706, acc 0.96875\n",
      "2016-03-01T18:11:34.051804: step 11100, loss 0.22466, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:11:40.118175: step 11101, loss 0.384953, acc 0.840131\n",
      "2016-03-01T18:11:48.895037: step 11150, loss 0.181674, acc 0.953125\n",
      "2016-03-01T18:11:57.719724: step 11200, loss 0.226704, acc 0.890625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:12:03.686514: step 11201, loss 0.382846, acc 0.840131\n",
      "2016-03-01T18:12:12.330458: step 11250, loss 0.0904501, acc 0.984375\n",
      "2016-03-01T18:12:21.218936: step 11300, loss 0.144898, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:12:27.329628: step 11301, loss 0.384883, acc 0.838256\n",
      "2016-03-01T18:12:36.011184: step 11350, loss 0.113176, acc 0.96875\n",
      "2016-03-01T18:12:44.779704: step 11400, loss 0.204217, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:12:50.262106: step 11401, loss 0.37996, acc 0.841538\n",
      "2016-03-01T18:12:58.931062: step 11450, loss 0.142312, acc 0.953125\n",
      "2016-03-01T18:13:07.772337: step 11500, loss 0.106082, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:13:13.719737: step 11501, loss 0.377436, acc 0.841538\n",
      "2016-03-01T18:13:22.192392: step 11550, loss 0.093901, acc 0.96875\n",
      "2016-03-01T18:13:30.964114: step 11600, loss 0.168146, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:13:36.574101: step 11601, loss 0.378061, acc 0.844351\n",
      "2016-03-01T18:13:45.121772: step 11650, loss 0.0788015, acc 0.984375\n",
      "2016-03-01T18:13:53.869201: step 11700, loss 0.109818, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:13:59.600923: step 11701, loss 0.374715, acc 0.844351\n",
      "2016-03-01T18:14:08.321591: step 11750, loss 0.0549022, acc 1\n",
      "2016-03-01T18:14:17.068346: step 11800, loss 0.117608, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:14:23.031418: step 11801, loss 0.373634, acc 0.845288\n",
      "2016-03-01T18:14:31.684008: step 11850, loss 0.124092, acc 0.9375\n",
      "2016-03-01T18:14:40.593482: step 11900, loss 0.11893, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:14:46.338181: step 11901, loss 0.372755, acc 0.849508\n",
      "2016-03-01T18:14:54.926958: step 11950, loss 0.15428, acc 0.9375\n",
      "2016-03-01T18:15:03.857108: step 12000, loss 0.187268, acc 0.90625\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:15:09.812124: step 12001, loss 0.370323, acc 0.846695\n",
      "2016-03-01T18:15:18.470563: step 12050, loss 0.065418, acc 0.984375\n",
      "2016-03-01T18:15:27.234124: step 12100, loss 0.162076, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:15:32.853352: step 12101, loss 0.368538, acc 0.846695\n",
      "2016-03-01T18:15:41.662881: step 12150, loss 0.145139, acc 0.921875\n",
      "2016-03-01T18:15:50.360839: step 12200, loss 0.12849, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:16:49.474837: step 12201, loss 0.368595, acc 0.851383\n",
      "2016-03-01T18:16:59.166310: step 12250, loss 0.0900186, acc 0.953125\n",
      "2016-03-01T18:17:08.350074: step 12300, loss 0.0648854, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:17:14.132176: step 12301, loss 0.367178, acc 0.849977\n",
      "2016-03-01T18:17:22.746153: step 12350, loss 0.0983, acc 0.96875\n",
      "2016-03-01T18:17:31.598624: step 12400, loss 0.129455, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:17:37.504760: step 12401, loss 0.363728, acc 0.850914\n",
      "2016-03-01T18:17:46.228138: step 12450, loss 0.0922734, acc 0.984375\n",
      "2016-03-01T18:17:55.027030: step 12500, loss 0.0902566, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:18:00.799397: step 12501, loss 0.362817, acc 0.851852\n",
      "2016-03-01T18:18:09.541511: step 12550, loss 0.115148, acc 0.9375\n",
      "2016-03-01T18:18:18.342925: step 12600, loss 0.0869289, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:18:24.282347: step 12601, loss 0.360636, acc 0.853727\n",
      "2016-03-01T18:18:32.956631: step 12650, loss 0.119108, acc 0.953125\n",
      "2016-03-01T18:18:41.867934: step 12700, loss 0.0436423, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:18:47.924394: step 12701, loss 0.358587, acc 0.855602\n",
      "2016-03-01T18:18:56.499172: step 12750, loss 0.0905526, acc 0.953125\n",
      "2016-03-01T18:19:05.377621: step 12800, loss 0.0923555, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:19:11.416321: step 12801, loss 0.355392, acc 0.857009\n",
      "2016-03-01T18:19:20.076423: step 12850, loss 0.15604, acc 0.9375\n",
      "2016-03-01T18:19:28.833952: step 12900, loss 0.0740172, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:19:34.724177: step 12901, loss 0.352923, acc 0.857478\n",
      "2016-03-01T18:19:43.376511: step 12950, loss 0.0830232, acc 0.96875\n",
      "2016-03-01T18:19:52.055759: step 13000, loss 0.0480719, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:19:57.843003: step 13001, loss 0.351405, acc 0.860759\n",
      "2016-03-01T18:20:06.509070: step 13050, loss 0.089678, acc 0.96875\n",
      "2016-03-01T18:20:15.261474: step 13100, loss 0.0968097, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:20:21.174985: step 13101, loss 0.350102, acc 0.859822\n",
      "2016-03-01T18:20:29.747460: step 13150, loss 0.114886, acc 0.953125\n",
      "2016-03-01T18:20:38.584835: step 13200, loss 0.0866431, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:20:44.535322: step 13201, loss 0.352524, acc 0.860291\n",
      "2016-03-01T18:20:53.195776: step 13250, loss 0.0717006, acc 0.96875\n",
      "2016-03-01T18:21:01.947793: step 13300, loss 0.0844187, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:21:07.753061: step 13301, loss 0.345491, acc 0.864979\n",
      "2016-03-01T18:21:16.394213: step 13350, loss 0.0768041, acc 0.96875\n",
      "2016-03-01T18:21:25.135159: step 13400, loss 0.0833343, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:21:31.230160: step 13401, loss 0.344656, acc 0.862635\n",
      "2016-03-01T18:21:39.880736: step 13450, loss 0.0955082, acc 0.953125\n",
      "2016-03-01T18:21:48.733406: step 13500, loss 0.107493, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:21:54.420389: step 13501, loss 0.34544, acc 0.862166\n",
      "2016-03-01T18:22:03.002308: step 13550, loss 0.126883, acc 0.9375\n",
      "2016-03-01T18:22:11.747326: step 13600, loss 0.0725852, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:22:17.557246: step 13601, loss 0.342257, acc 0.864979\n",
      "2016-03-01T18:22:26.211740: step 13650, loss 0.0885189, acc 0.953125\n",
      "2016-03-01T18:22:35.847101: step 13700, loss 0.0428508, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:22:42.837769: step 13701, loss 0.337007, acc 0.871542\n",
      "2016-03-01T18:22:54.132356: step 13750, loss 0.0917661, acc 0.953125\n",
      "2016-03-01T18:23:04.299234: step 13800, loss 0.155747, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:23:10.139228: step 13801, loss 0.335335, acc 0.872949\n",
      "2016-03-01T18:23:18.686360: step 13850, loss 0.0747473, acc 0.984375\n",
      "2016-03-01T18:23:27.540427: step 13900, loss 0.0574846, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:23:33.303944: step 13901, loss 0.332575, acc 0.869198\n",
      "2016-03-01T18:23:41.889661: step 13950, loss 0.042217, acc 1\n",
      "2016-03-01T18:23:50.717346: step 14000, loss 0.162047, acc 0.9375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:23:56.594337: step 14001, loss 0.332131, acc 0.868261\n",
      "2016-03-01T18:24:05.208344: step 14050, loss 0.0693072, acc 0.984375\n",
      "2016-03-01T18:24:13.964331: step 14100, loss 0.0205571, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:24:19.797654: step 14101, loss 0.330981, acc 0.870136\n",
      "2016-03-01T18:24:28.504255: step 14150, loss 0.0652007, acc 0.984375\n",
      "2016-03-01T18:24:37.242601: step 14200, loss 0.0978902, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:24:43.131831: step 14201, loss 0.326453, acc 0.875762\n",
      "2016-03-01T18:24:51.738655: step 14250, loss 0.0337132, acc 1\n",
      "2016-03-01T18:25:00.533136: step 14300, loss 0.0608874, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:25:06.313466: step 14301, loss 0.323779, acc 0.877637\n",
      "2016-03-01T18:25:14.822816: step 14350, loss 0.0694622, acc 0.96875\n",
      "2016-03-01T18:25:23.760940: step 14400, loss 0.102418, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:25:29.606419: step 14401, loss 0.321737, acc 0.878575\n",
      "2016-03-01T18:25:38.221294: step 14450, loss 0.0525928, acc 0.984375\n",
      "2016-03-01T18:25:46.976781: step 14500, loss 0.145657, acc 0.921875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:25:52.833135: step 14501, loss 0.319128, acc 0.877637\n",
      "2016-03-01T18:26:01.472161: step 14550, loss 0.0814539, acc 0.953125\n",
      "2016-03-01T18:26:10.229889: step 14600, loss 0.0732257, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:26:16.228668: step 14601, loss 0.316102, acc 0.882325\n",
      "2016-03-01T18:26:24.828406: step 14650, loss 0.115499, acc 0.953125\n",
      "2016-03-01T18:26:33.607896: step 14700, loss 0.0274842, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:26:39.404672: step 14701, loss 0.313751, acc 0.879981\n",
      "2016-03-01T18:26:48.056779: step 14750, loss 0.0249868, acc 1\n",
      "2016-03-01T18:26:56.744571: step 14800, loss 0.0817915, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:27:02.518118: step 14801, loss 0.312667, acc 0.883263\n",
      "2016-03-01T18:27:11.249468: step 14850, loss 0.0580401, acc 0.984375\n",
      "2016-03-01T18:27:19.927716: step 14900, loss 0.0924955, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:27:25.850097: step 14901, loss 0.308993, acc 0.882325\n",
      "2016-03-01T18:27:34.550244: step 14950, loss 0.0423092, acc 0.984375\n",
      "2016-03-01T18:27:43.342433: step 15000, loss 0.0698431, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:27:49.365493: step 15001, loss 0.307364, acc 0.884201\n",
      "2016-03-01T18:27:57.926124: step 15050, loss 0.028673, acc 1\n",
      "2016-03-01T18:28:06.733583: step 15100, loss 0.0325033, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:28:12.348912: step 15101, loss 0.305858, acc 0.884669\n",
      "2016-03-01T18:28:21.058510: step 15150, loss 0.0459626, acc 1\n",
      "2016-03-01T18:28:29.783030: step 15200, loss 0.0739147, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:28:35.749836: step 15201, loss 0.302433, acc 0.885607\n",
      "2016-03-01T18:28:44.487591: step 15250, loss 0.0240909, acc 1\n",
      "2016-03-01T18:28:53.178921: step 15300, loss 0.0403049, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:28:59.241832: step 15301, loss 0.301793, acc 0.886076\n",
      "2016-03-01T18:29:07.813432: step 15350, loss 0.0385492, acc 1\n",
      "2016-03-01T18:29:16.653640: step 15400, loss 0.0291724, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:29:22.656833: step 15401, loss 0.298349, acc 0.889827\n",
      "2016-03-01T18:29:31.184473: step 15450, loss 0.0676178, acc 0.96875\n",
      "2016-03-01T18:29:40.057236: step 15500, loss 0.0940802, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:29:45.838398: step 15501, loss 0.296062, acc 0.892639\n",
      "2016-03-01T18:29:54.528048: step 15550, loss 0.0326121, acc 1\n",
      "2016-03-01T18:30:03.351471: step 15600, loss 0.0191323, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:30:09.273371: step 15601, loss 0.297378, acc 0.886545\n",
      "2016-03-01T18:30:17.876521: step 15650, loss 0.0628347, acc 0.96875\n",
      "2016-03-01T18:30:26.637451: step 15700, loss 0.00708793, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:30:32.440849: step 15701, loss 0.292477, acc 0.890295\n",
      "2016-03-01T18:30:41.161113: step 15750, loss 0.0543768, acc 0.984375\n",
      "2016-03-01T18:30:50.012437: step 15800, loss 0.0226315, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:30:56.076719: step 15801, loss 0.291129, acc 0.891702\n",
      "2016-03-01T18:31:04.627384: step 15850, loss 0.0588866, acc 0.96875\n",
      "2016-03-01T18:31:13.437205: step 15900, loss 0.0223338, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:31:19.787134: step 15901, loss 0.286011, acc 0.893108\n",
      "2016-03-01T18:31:28.398879: step 15950, loss 0.0663022, acc 0.96875\n",
      "2016-03-01T18:31:37.131138: step 16000, loss 0.0488183, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:31:43.225693: step 16001, loss 0.283527, acc 0.892639\n",
      "2016-03-01T18:31:51.918927: step 16050, loss 0.0835272, acc 0.96875\n",
      "2016-03-01T18:32:00.777680: step 16100, loss 0.0249046, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:32:06.656628: step 16101, loss 0.28081, acc 0.894515\n",
      "2016-03-01T18:32:15.241843: step 16150, loss 0.0403957, acc 1\n",
      "2016-03-01T18:32:24.013263: step 16200, loss 0.047371, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:32:30.027550: step 16201, loss 0.278345, acc 0.895452\n",
      "2016-03-01T18:32:38.546093: step 16250, loss 0.0287826, acc 0.984375\n",
      "2016-03-01T18:32:47.385373: step 16300, loss 0.0606214, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:32:53.303234: step 16301, loss 0.274091, acc 0.895452\n",
      "2016-03-01T18:33:01.976770: step 16350, loss 0.0443025, acc 0.984375\n",
      "2016-03-01T18:33:10.634001: step 16400, loss 0.0148063, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:33:16.293872: step 16401, loss 0.270566, acc 0.897328\n",
      "2016-03-01T18:33:24.971926: step 16450, loss 0.0431292, acc 0.984375\n",
      "2016-03-01T18:34:09.784515: step 16500, loss 0.0566864, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:34:15.963409: step 16501, loss 0.267124, acc 0.898734\n",
      "2016-03-01T18:34:26.762805: step 16550, loss 0.0634529, acc 0.96875\n",
      "2016-03-01T18:34:38.814528: step 16600, loss 0.187983, acc 0.953125\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:34:45.311189: step 16601, loss 0.263593, acc 0.899672\n",
      "2016-03-01T18:34:54.480620: step 16650, loss 0.0194331, acc 0.984375\n",
      "2016-03-01T18:35:03.813161: step 16700, loss 0.0226994, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:35:09.733097: step 16701, loss 0.261833, acc 0.902954\n",
      "2016-03-01T18:35:18.990154: step 16750, loss 0.0482836, acc 0.984375\n",
      "2016-03-01T18:35:27.687949: step 16800, loss 0.0497081, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:35:34.095550: step 16801, loss 0.261021, acc 0.900141\n",
      "2016-03-01T18:35:43.445560: step 16850, loss 0.0396286, acc 0.984375\n",
      "2016-03-01T18:35:54.002759: step 16900, loss 0.016906, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:36:00.345935: step 16901, loss 0.260421, acc 0.902485\n",
      "2016-03-01T18:36:09.943850: step 16950, loss 0.0173115, acc 1\n",
      "2016-03-01T18:36:20.216098: step 17000, loss 0.01543, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:36:26.654346: step 17001, loss 0.255497, acc 0.904829\n",
      "2016-03-01T18:36:36.353406: step 17050, loss 0.0189966, acc 1\n",
      "2016-03-01T18:36:46.143460: step 17100, loss 0.0249438, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:36:52.496892: step 17101, loss 0.250897, acc 0.905298\n",
      "2016-03-01T18:37:02.581077: step 17150, loss 0.026356, acc 0.984375\n",
      "2016-03-01T18:37:11.345601: step 17200, loss 0.0536101, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:37:17.109389: step 17201, loss 0.249725, acc 0.911392\n",
      "2016-03-01T18:37:25.839132: step 17250, loss 0.041414, acc 0.984375\n",
      "2016-03-01T18:37:34.725902: step 17300, loss 0.0390483, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:37:40.514492: step 17301, loss 0.24601, acc 0.908111\n",
      "2016-03-01T18:37:49.151277: step 17350, loss 0.0145512, acc 1\n",
      "2016-03-01T18:37:58.097152: step 17400, loss 0.069594, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:38:03.735043: step 17401, loss 0.242432, acc 0.911392\n",
      "2016-03-01T18:38:12.429454: step 17450, loss 0.041422, acc 0.984375\n",
      "2016-03-01T18:38:21.133785: step 17500, loss 0.0450659, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:38:26.768975: step 17501, loss 0.241631, acc 0.909986\n",
      "2016-03-01T18:38:35.495672: step 17550, loss 0.0379484, acc 0.984375\n",
      "2016-03-01T18:38:44.292165: step 17600, loss 0.0204208, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:38:50.075501: step 17601, loss 0.236225, acc 0.913268\n",
      "2016-03-01T18:38:58.771762: step 17650, loss 0.0184039, acc 1\n",
      "2016-03-01T18:39:07.714848: step 17700, loss 0.0123965, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:39:13.528393: step 17701, loss 0.23442, acc 0.914674\n",
      "2016-03-01T18:39:22.231133: step 17750, loss 0.0142317, acc 1\n",
      "2016-03-01T18:39:31.065428: step 17800, loss 0.0186594, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:39:36.750323: step 17801, loss 0.230726, acc 0.916081\n",
      "2016-03-01T18:39:45.479664: step 17850, loss 0.0201001, acc 0.984375\n",
      "2016-03-01T18:39:54.525799: step 17900, loss 0.0287646, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:40:00.275904: step 17901, loss 0.228007, acc 0.918425\n",
      "2016-03-01T18:40:09.051361: step 17950, loss 0.0582626, acc 0.984375\n",
      "2016-03-01T18:40:17.814282: step 18000, loss 0.00418001, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:40:23.504474: step 18001, loss 0.224228, acc 0.919362\n",
      "2016-03-01T18:40:32.144457: step 18050, loss 0.00876463, acc 1\n",
      "2016-03-01T18:40:40.964597: step 18100, loss 0.0104342, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:40:47.129676: step 18101, loss 0.22096, acc 0.921238\n",
      "2016-03-01T18:40:55.789570: step 18150, loss 0.0174095, acc 1\n",
      "2016-03-01T18:41:04.655527: step 18200, loss 0.0159901, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:41:10.306753: step 18201, loss 0.216971, acc 0.924519\n",
      "2016-03-01T18:41:19.036252: step 18250, loss 0.0137355, acc 1\n",
      "2016-03-01T18:41:27.771398: step 18300, loss 0.0125732, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:41:33.506692: step 18301, loss 0.214882, acc 0.921238\n",
      "2016-03-01T18:41:42.194258: step 18350, loss 0.064841, acc 0.984375\n",
      "2016-03-01T18:41:51.071360: step 18400, loss 0.017676, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:41:56.794178: step 18401, loss 0.210865, acc 0.924988\n",
      "2016-03-01T18:42:05.413130: step 18450, loss 0.0203481, acc 1\n",
      "2016-03-01T18:42:14.305549: step 18500, loss 0.0458386, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:42:20.069542: step 18501, loss 0.207737, acc 0.925926\n",
      "2016-03-01T18:42:28.674234: step 18550, loss 0.0283383, acc 0.984375\n",
      "2016-03-01T18:42:37.551614: step 18600, loss 0.0223296, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:42:43.267863: step 18601, loss 0.205961, acc 0.927801\n",
      "2016-03-01T18:42:51.929233: step 18650, loss 0.00685377, acc 1\n",
      "2016-03-01T18:43:00.702657: step 18700, loss 0.0252754, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:43:06.313624: step 18701, loss 0.204473, acc 0.925457\n",
      "2016-03-01T18:43:15.001808: step 18750, loss 0.0071758, acc 1\n",
      "2016-03-01T18:43:23.841725: step 18800, loss 0.0242307, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:43:29.776686: step 18801, loss 0.199279, acc 0.92827\n",
      "2016-03-01T18:43:38.359538: step 18850, loss 0.00866433, acc 1\n",
      "2016-03-01T18:43:47.218584: step 18900, loss 0.0360427, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:43:52.981673: step 18901, loss 0.196981, acc 0.930614\n",
      "2016-03-01T18:44:01.547779: step 18950, loss 0.00503003, acc 1\n",
      "2016-03-01T18:44:10.406636: step 19000, loss 0.0157218, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:44:16.238374: step 19001, loss 0.193163, acc 0.932958\n",
      "2016-03-01T18:44:24.910156: step 19050, loss 0.0261733, acc 0.984375\n",
      "2016-03-01T18:44:33.819998: step 19100, loss 0.066934, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:44:39.562852: step 19101, loss 0.190276, acc 0.932489\n",
      "2016-03-01T18:44:48.316109: step 19150, loss 0.0196242, acc 1\n",
      "2016-03-01T18:44:57.199001: step 19200, loss 0.0104698, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:45:02.899676: step 19201, loss 0.189886, acc 0.932958\n",
      "2016-03-01T18:45:11.639360: step 19250, loss 0.010262, acc 1\n",
      "2016-03-01T18:45:20.456513: step 19300, loss 0.0166467, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:45:26.279953: step 19301, loss 0.185825, acc 0.93624\n",
      "2016-03-01T18:45:35.003477: step 19350, loss 0.0154102, acc 1\n",
      "2016-03-01T18:45:43.867725: step 19400, loss 0.0193695, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:45:49.560988: step 19401, loss 0.183635, acc 0.932489\n",
      "2016-03-01T18:45:58.288151: step 19450, loss 0.0484404, acc 0.984375\n",
      "2016-03-01T18:46:07.135601: step 19500, loss 0.0092784, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:46:12.858982: step 19501, loss 0.180544, acc 0.938584\n",
      "2016-03-01T18:46:21.539043: step 19550, loss 0.0123752, acc 1\n",
      "2016-03-01T18:46:30.347856: step 19600, loss 0.046217, acc 0.96875\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:46:36.172071: step 19601, loss 0.178138, acc 0.938115\n",
      "2016-03-01T18:46:44.800716: step 19650, loss 0.00627697, acc 1\n",
      "2016-03-01T18:46:53.697505: step 19700, loss 0.012179, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:46:59.483145: step 19701, loss 0.173834, acc 0.939053\n",
      "2016-03-01T18:47:08.197814: step 19750, loss 0.00278454, acc 1\n",
      "2016-03-01T18:47:16.940569: step 19800, loss 0.00344897, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:47:22.583777: step 19801, loss 0.170652, acc 0.939522\n",
      "2016-03-01T18:47:31.304976: step 19850, loss 0.0114125, acc 1\n",
      "2016-03-01T18:47:40.080805: step 19900, loss 0.0216866, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:47:45.817191: step 19901, loss 0.168062, acc 0.941397\n",
      "2016-03-01T18:47:54.479892: step 19950, loss 0.0187919, acc 1\n",
      "2016-03-01T18:48:03.993964: step 20000, loss 0.0286182, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:48:09.795973: step 20001, loss 0.167351, acc 0.939991\n",
      "2016-03-01T18:48:18.741999: step 20050, loss 0.0156684, acc 1\n",
      "2016-03-01T18:48:28.174188: step 20100, loss 0.0111066, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:48:34.386054: step 20101, loss 0.161247, acc 0.943741\n",
      "2016-03-01T18:48:44.528755: step 20150, loss 0.0177491, acc 1\n",
      "2016-03-01T18:48:54.106400: step 20200, loss 0.0151412, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:49:00.093680: step 20201, loss 0.158315, acc 0.941866\n",
      "2016-03-01T18:49:09.235754: step 20250, loss 0.0159461, acc 1\n",
      "2016-03-01T18:49:18.246541: step 20300, loss 0.0120041, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:49:24.334719: step 20301, loss 0.154661, acc 0.944679\n",
      "2016-03-01T18:49:33.313589: step 20350, loss 0.00445607, acc 1\n",
      "2016-03-01T18:49:42.496120: step 20400, loss 0.00890966, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:49:48.272089: step 20401, loss 0.152202, acc 0.946554\n",
      "2016-03-01T18:49:57.577165: step 20450, loss 0.019682, acc 0.984375\n",
      "2016-03-01T18:50:07.541906: step 20500, loss 0.0116137, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:50:13.409389: step 20501, loss 0.149586, acc 0.945148\n",
      "2016-03-01T18:50:22.312330: step 20550, loss 0.00623617, acc 1\n",
      "2016-03-01T18:50:31.351394: step 20600, loss 0.00802735, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:50:38.019685: step 20601, loss 0.145231, acc 0.949836\n",
      "2016-03-01T18:50:48.221192: step 20650, loss 0.00980384, acc 1\n",
      "2016-03-01T18:50:58.296669: step 20700, loss 0.0106867, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:51:04.082065: step 20701, loss 0.14329, acc 0.948429\n",
      "2016-03-01T18:51:12.935494: step 20750, loss 0.0232028, acc 0.984375\n",
      "2016-03-01T18:51:23.007918: step 20800, loss 0.00861571, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:51:29.743563: step 20801, loss 0.1399, acc 0.951242\n",
      "2016-03-01T18:51:39.245864: step 20850, loss 0.0111241, acc 1\n",
      "2016-03-01T18:51:48.603346: step 20900, loss 0.00482943, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:51:54.594347: step 20901, loss 0.136366, acc 0.951711\n",
      "2016-03-01T18:52:03.889091: step 20950, loss 0.0204791, acc 1\n",
      "2016-03-01T18:52:13.437926: step 21000, loss 0.00708752, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:52:20.133569: step 21001, loss 0.132909, acc 0.953118\n",
      "2016-03-01T18:52:30.035500: step 21050, loss 0.0101414, acc 1\n",
      "2016-03-01T18:52:41.316090: step 21100, loss 0.00395656, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:52:47.069604: step 21101, loss 0.130813, acc 0.953587\n",
      "2016-03-01T18:52:56.526477: step 21150, loss 0.016527, acc 1\n",
      "2016-03-01T18:53:06.803097: step 21200, loss 0.00389312, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:53:14.322344: step 21201, loss 0.127857, acc 0.954524\n",
      "2016-03-01T18:53:24.981060: step 21250, loss 0.00296417, acc 1\n",
      "2016-03-01T18:53:34.807825: step 21300, loss 0.00825768, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:53:40.605224: step 21301, loss 0.125112, acc 0.955931\n",
      "2016-03-01T18:53:49.321650: step 21350, loss 0.00678196, acc 1\n",
      "2016-03-01T18:53:58.473922: step 21400, loss 0.0177608, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:54:04.281623: step 21401, loss 0.122467, acc 0.954524\n",
      "2016-03-01T18:54:12.978515: step 21450, loss 0.00575932, acc 1\n",
      "2016-03-01T18:54:21.839599: step 21500, loss 0.00278016, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:54:27.567543: step 21501, loss 0.120856, acc 0.958275\n",
      "2016-03-01T18:54:36.177740: step 21550, loss 0.0098916, acc 1\n",
      "2016-03-01T18:54:44.957034: step 21600, loss 0.00651332, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:54:50.793958: step 21601, loss 0.118116, acc 0.958744\n",
      "2016-03-01T18:54:59.491420: step 21650, loss 0.00738169, acc 1\n",
      "2016-03-01T18:55:08.367691: step 21700, loss 0.00138351, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:55:14.028965: step 21701, loss 0.116106, acc 0.96015\n",
      "2016-03-01T18:55:22.771117: step 21750, loss 0.00672281, acc 1\n",
      "2016-03-01T18:55:31.550255: step 21800, loss 0.00957562, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:55:37.175614: step 21801, loss 0.113869, acc 0.960619\n",
      "2016-03-01T18:55:45.892053: step 21850, loss 0.00284345, acc 1\n",
      "2016-03-01T18:55:54.738551: step 21900, loss 0.00404861, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:56:00.394182: step 21901, loss 0.111418, acc 0.961088\n",
      "2016-03-01T18:56:08.966720: step 21950, loss 0.00745716, acc 1\n",
      "2016-03-01T18:56:17.877610: step 22000, loss 0.0118647, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:56:23.615022: step 22001, loss 0.108993, acc 0.961556\n",
      "2016-03-01T18:56:32.280609: step 22050, loss 0.0119263, acc 1\n",
      "2016-03-01T18:56:41.054744: step 22100, loss 0.00512983, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:56:46.765290: step 22101, loss 0.106838, acc 0.964369\n",
      "2016-03-01T18:56:55.497702: step 22150, loss 0.0156931, acc 1\n",
      "2016-03-01T18:57:04.334666: step 22200, loss 0.0098918, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:57:10.106913: step 22201, loss 0.104381, acc 0.966714\n",
      "2016-03-01T18:57:18.733600: step 22250, loss 0.00850157, acc 1\n",
      "2016-03-01T18:57:27.621220: step 22300, loss 0.00317666, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:57:33.402198: step 22301, loss 0.101653, acc 0.967651\n",
      "2016-03-01T18:57:41.975153: step 22350, loss 0.0377096, acc 0.984375\n",
      "2016-03-01T18:57:50.803014: step 22400, loss 0.0141719, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:57:56.544386: step 22401, loss 0.0988527, acc 0.96812\n",
      "2016-03-01T18:58:05.237712: step 22450, loss 0.00519503, acc 1\n",
      "2016-03-01T18:58:13.930905: step 22500, loss 0.0121295, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:58:19.662181: step 22501, loss 0.0985964, acc 0.965776\n",
      "2016-03-01T18:58:28.338087: step 22550, loss 0.0187391, acc 1\n",
      "2016-03-01T18:58:37.264687: step 22600, loss 0.0159434, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:58:43.099052: step 22601, loss 0.0937938, acc 0.970464\n",
      "2016-03-01T18:58:51.681799: step 22650, loss 0.00829879, acc 1\n",
      "2016-03-01T18:59:00.502614: step 22700, loss 0.0332672, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:59:06.497767: step 22701, loss 0.0920553, acc 0.971402\n",
      "2016-03-01T18:59:15.141910: step 22750, loss 0.00432508, acc 1\n",
      "2016-03-01T18:59:24.044612: step 22800, loss 0.0102349, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:59:29.986168: step 22801, loss 0.092478, acc 0.969995\n",
      "2016-03-01T18:59:38.758864: step 22850, loss 0.0067012, acc 1\n",
      "2016-03-01T18:59:47.585489: step 22900, loss 0.0139157, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T18:59:53.558440: step 22901, loss 0.0877655, acc 0.971871\n",
      "2016-03-01T19:00:02.234035: step 22950, loss 0.00588236, acc 1\n",
      "2016-03-01T19:00:12.196451: step 23000, loss 0.00253233, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:00:18.815851: step 23001, loss 0.0859342, acc 0.972339\n",
      "2016-03-01T19:00:29.277162: step 23050, loss 0.0215206, acc 1\n",
      "2016-03-01T19:00:39.135775: step 23100, loss 0.0017793, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:00:45.828329: step 23101, loss 0.0838293, acc 0.972339\n",
      "2016-03-01T19:00:54.853064: step 23150, loss 0.00171693, acc 1\n",
      "2016-03-01T19:01:04.893618: step 23200, loss 0.0021171, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:01:10.956933: step 23201, loss 0.0808373, acc 0.974684\n",
      "2016-03-01T19:01:20.327600: step 23250, loss 0.0122397, acc 1\n",
      "2016-03-01T19:01:29.708163: step 23300, loss 0.00372396, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:01:35.630463: step 23301, loss 0.0788011, acc 0.97609\n",
      "2016-03-01T19:01:45.230012: step 23350, loss 0.00822647, acc 1\n",
      "2016-03-01T19:01:55.178778: step 23400, loss 0.00241283, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:02:01.518975: step 23401, loss 0.0766123, acc 0.975621\n",
      "2016-03-01T19:02:10.836231: step 23450, loss 0.00754715, acc 1\n",
      "2016-03-01T19:02:20.279129: step 23500, loss 0.00740279, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:02:25.845893: step 23501, loss 0.0747533, acc 0.975621\n",
      "2016-03-01T19:02:34.644154: step 23550, loss 0.00925961, acc 1\n",
      "2016-03-01T19:02:44.271457: step 23600, loss 0.0062611, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:02:50.003295: step 23601, loss 0.0721632, acc 0.977028\n",
      "2016-03-01T19:03:00.300716: step 23650, loss 0.00379228, acc 1\n",
      "2016-03-01T19:03:10.051481: step 23700, loss 0.00299066, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:03:16.912613: step 23701, loss 0.069569, acc 0.977497\n",
      "2016-03-01T19:03:26.055295: step 23750, loss 0.0204337, acc 0.984375\n",
      "2016-03-01T19:03:37.007064: step 23800, loss 0.00462571, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:03:43.044849: step 23801, loss 0.0683394, acc 0.978903\n",
      "2016-03-01T19:03:52.532312: step 23850, loss 0.00176622, acc 1\n",
      "2016-03-01T19:04:04.887813: step 23900, loss 0.022055, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:04:11.594732: step 23901, loss 0.0679812, acc 0.978434\n",
      "2016-03-01T19:04:23.261279: step 23950, loss 0.00460472, acc 1\n",
      "2016-03-01T19:04:33.229741: step 24000, loss 0.00433114, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:04:39.402440: step 24001, loss 0.0662487, acc 0.980778\n",
      "2016-03-01T19:04:50.987414: step 24050, loss 0.0104443, acc 1\n",
      "2016-03-01T19:05:02.210788: step 24100, loss 0.00386967, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:05:08.312064: step 24101, loss 0.0651041, acc 0.981716\n",
      "2016-03-01T19:05:18.624887: step 24150, loss 0.00278377, acc 1\n",
      "2016-03-01T19:05:29.772798: step 24200, loss 0.00160537, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:05:36.581098: step 24201, loss 0.061642, acc 0.981247\n",
      "2016-03-01T19:05:47.052687: step 24250, loss 0.00166445, acc 1\n",
      "2016-03-01T19:05:57.128742: step 24300, loss 0.010506, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:06:04.004608: step 24301, loss 0.0707428, acc 0.977497\n",
      "2016-03-01T19:06:14.254404: step 24350, loss 0.0106079, acc 1\n",
      "2016-03-01T19:06:24.248374: step 24400, loss 0.00765963, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:06:31.100148: step 24401, loss 0.0590434, acc 0.980778\n",
      "2016-03-01T19:06:42.845224: step 24450, loss 0.00608945, acc 1\n",
      "2016-03-01T19:06:52.255285: step 24500, loss 0.00192853, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:06:58.100456: step 24501, loss 0.0572862, acc 0.982185\n",
      "2016-03-01T19:07:08.015903: step 24550, loss 0.0224207, acc 0.984375\n",
      "2016-03-01T19:07:19.874058: step 24600, loss 0.00615209, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:07:27.308083: step 24601, loss 0.0566341, acc 0.984529\n",
      "2016-03-01T19:07:37.993486: step 24650, loss 0.00242115, acc 1\n",
      "2016-03-01T19:07:48.484688: step 24700, loss 0.005869, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:07:54.978787: step 24701, loss 0.0540193, acc 0.985935\n",
      "2016-03-01T19:08:05.012676: step 24750, loss 0.000931765, acc 1\n",
      "2016-03-01T19:08:15.248994: step 24800, loss 0.00376267, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:08:21.722610: step 24801, loss 0.0530198, acc 0.984529\n",
      "2016-03-01T19:08:32.708910: step 24850, loss 0.0224179, acc 0.984375\n",
      "2016-03-01T19:08:42.812665: step 24900, loss 0.00225467, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:08:49.114103: step 24901, loss 0.0511259, acc 0.985935\n",
      "2016-03-01T19:08:59.446171: step 24950, loss 0.00178506, acc 1\n",
      "2016-03-01T19:09:09.072312: step 25000, loss 0.00319056, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:09:14.817544: step 25001, loss 0.0507869, acc 0.986873\n",
      "2016-03-01T19:09:25.041646: step 25050, loss 0.00446949, acc 1\n",
      "2016-03-01T19:09:34.585152: step 25100, loss 0.00161115, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:09:40.345974: step 25101, loss 0.0476252, acc 0.986873\n",
      "2016-03-01T19:09:49.458239: step 25150, loss 0.00539605, acc 1\n",
      "2016-03-01T19:09:59.123950: step 25200, loss 0.00463818, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:10:05.579594: step 25201, loss 0.0465001, acc 0.988279\n",
      "2016-03-01T19:10:15.469198: step 25250, loss 0.00260373, acc 1\n",
      "2016-03-01T19:10:24.400773: step 25300, loss 0.00332495, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:10:30.911292: step 25301, loss 0.0443943, acc 0.990155\n",
      "2016-03-01T19:10:39.645145: step 25350, loss 0.0101282, acc 1\n",
      "2016-03-01T19:10:48.895829: step 25400, loss 0.00176806, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:10:57.205245: step 25401, loss 0.0434241, acc 0.990624\n",
      "2016-03-01T19:11:08.680868: step 25450, loss 0.0055608, acc 1\n",
      "2016-03-01T19:11:20.083293: step 25500, loss 0.00267265, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:11:28.168192: step 25501, loss 0.0418256, acc 0.991092\n",
      "2016-03-01T19:11:39.223721: step 25550, loss 0.0743626, acc 0.984375\n",
      "2016-03-01T19:11:51.400995: step 25600, loss 0.00236637, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:11:59.132801: step 25601, loss 0.0409894, acc 0.989686\n",
      "2016-03-01T19:12:08.810334: step 25650, loss 0.00120528, acc 1\n",
      "2016-03-01T19:12:19.185438: step 25700, loss 0.00163645, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:12:25.074546: step 25701, loss 0.0393847, acc 0.991561\n",
      "2016-03-01T19:12:34.018954: step 25750, loss 0.00352211, acc 1\n",
      "2016-03-01T19:12:44.163176: step 25800, loss 0.00092645, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:12:50.576859: step 25801, loss 0.038905, acc 0.991561\n",
      "2016-03-01T19:13:00.886179: step 25850, loss 0.00249975, acc 1\n",
      "2016-03-01T19:13:11.176008: step 25900, loss 0.00178386, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:13:17.817824: step 25901, loss 0.0373168, acc 0.992499\n",
      "2016-03-01T19:13:27.753550: step 25950, loss 0.00352639, acc 1\n",
      "2016-03-01T19:13:37.624915: step 26000, loss 0.000814658, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:13:44.066254: step 26001, loss 0.0388153, acc 0.990624\n",
      "2016-03-01T19:13:54.395654: step 26050, loss 0.00602194, acc 1\n",
      "2016-03-01T19:14:05.752433: step 26100, loss 0.00599283, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:14:11.948148: step 26101, loss 0.0357981, acc 0.992499\n",
      "2016-03-01T19:14:21.266487: step 26150, loss 0.00799926, acc 1\n",
      "2016-03-01T19:14:31.011991: step 26200, loss 0.00381674, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:14:37.349798: step 26201, loss 0.0347166, acc 0.993436\n",
      "2016-03-01T19:14:47.116244: step 26250, loss 0.0102823, acc 1\n",
      "2016-03-01T19:14:56.708763: step 26300, loss 0.0162041, acc 0.984375\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:15:02.801975: step 26301, loss 0.0343034, acc 0.992968\n",
      "2016-03-01T19:15:12.583975: step 26350, loss 0.00111072, acc 1\n",
      "2016-03-01T19:15:23.461629: step 26400, loss 0.0053315, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:15:29.632632: step 26401, loss 0.0372674, acc 0.991561\n",
      "2016-03-01T19:15:39.158770: step 26450, loss 0.000556908, acc 1\n",
      "2016-03-01T19:15:48.435232: step 26500, loss 0.00408443, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:15:54.276227: step 26501, loss 0.0316667, acc 0.994374\n",
      "2016-03-01T19:16:03.275806: step 26550, loss 0.0028595, acc 1\n",
      "2016-03-01T19:16:12.823015: step 26600, loss 0.0015293, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:16:18.885345: step 26601, loss 0.0325761, acc 0.993436\n",
      "2016-03-01T19:16:28.370545: step 26650, loss 0.00143051, acc 1\n",
      "2016-03-01T19:16:38.083605: step 26700, loss 0.00303877, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:16:43.954809: step 26701, loss 0.0301073, acc 0.994374\n",
      "2016-03-01T19:16:54.272267: step 26750, loss 0.009229, acc 1\n",
      "2016-03-01T19:17:05.043224: step 26800, loss 0.00247369, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:17:11.863169: step 26801, loss 0.0294595, acc 0.994843\n",
      "2016-03-01T19:17:21.931483: step 26850, loss 0.0568537, acc 0.984375\n",
      "2016-03-01T19:17:32.613939: step 26900, loss 0.000952628, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:17:39.176751: step 26901, loss 0.0285167, acc 0.994843\n",
      "2016-03-01T19:17:48.850452: step 26950, loss 0.00245641, acc 1\n",
      "2016-03-01T19:17:58.032091: step 27000, loss 0.00102025, acc 1\n",
      "-----Test Accuracy-----\n",
      "2016-03-01T19:18:04.198818: step 27001, loss 0.0275976, acc 0.995781\n",
      "2016-03-01T19:18:13.500092: step 27050, loss 0.0170698, acc 0.984375\n",
      "----Training Finished-----\n",
      "2016-03-01T19:18:23.466121: step 27071, loss 0.0273262, acc 0.995312\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "filter_sizes = '3,4,5'\n",
    "num_filters = 128\n",
    "learning_rate = 1e-4\n",
    "keep_prob = 0.5\n",
    "l2_reg_lambda = 0\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False\n",
    "\n",
    "\n",
    "# Get Data\n",
    "X, y, vocabulary, vocabulary_inv = data_helpers2.load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "print 'Vocabulary Size: {}'.format(len(vocabulary))\n",
    "print 'Train / Test Split: {} / {}'.format(X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "# Lets Train\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    session_config = tf.ConfigProto(\n",
    "        allow_soft_placement = allow_soft_placement,\n",
    "        log_device_placement = log_device_placement)\n",
    "    \n",
    "    sess = tf.Session(config = session_config)\n",
    "\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length = X_train.shape[1],\n",
    "            num_classes = 2,\n",
    "            vocab_size = len(vocabulary),\n",
    "            embedding_size = embedding_dim,\n",
    "            filter_sizes = map(int, filter_sizes.split(\",\")),\n",
    "            num_filters = num_filters,\n",
    "            l2_reg_lambda = l2_reg_lambda)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable = False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_optimizer = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "    def train_step(batch_x, batch_y):\n",
    "        feed_dict = {cnn.x : batch_x, cnn.y_ : batch_y, cnn.keep_prob : keep_prob}\n",
    "        _, step, loss, accuracy = sess.run([train_optimizer, global_step, cnn.loss, cnn.accuracy], \n",
    "                                           feed_dict = feed_dict)\n",
    "        time_now = datetime.now().isoformat()\n",
    "        if step % 50 == 0:\n",
    "            print '{}: step {}, loss {:g}, acc {:g}'.format(time_now, step, loss, accuracy)\n",
    "        \n",
    "        \n",
    "    def test_step(batch_x, batch_y):\n",
    "        feed_dict = {cnn.x : batch_x, cnn.y_ : batch_y, cnn.keep_prob : 1.0}\n",
    "        _, step, loss, accuracy = sess.run([train_optimizer, global_step, cnn.loss, cnn.accuracy], \n",
    "                                           feed_dict = feed_dict)\n",
    "        time_now = datetime.now().isoformat()\n",
    "        print '{}: step {}, loss {:g}, acc {:g}'.format(time_now, step, loss, accuracy)\n",
    "        \n",
    "    def run():\n",
    "            batches = data_helpers2.batch_iter(zip(X_train, y_train), batch_size, num_epochs)\n",
    "            \n",
    "            for batch in batches:\n",
    "                batch_x, batch_y = zip(*batch)\n",
    "                train_step(batch_x, batch_y)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 100 == 0:\n",
    "                    print '-----Test Accuracy-----'\n",
    "                    test_step(X_test, y_test)\n",
    "            print '----Training Finished-----'\n",
    "            test_step(X_test, y_test)\n",
    "                \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final Results\n",
    "\n",
    "----Training Finished-----\n",
    "\n",
    "2016-03-01T19:18:23.466121: step 27071, loss 0.0273262, acc 0.995312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
